# Lint as: python3
"""Unit tests for datasets."""

from absl.testing import absltest
from absl.testing import parameterized
import jax
import jax.numpy as jnp
from scenic.dataset_lib import dataset_utils
from scenic.dataset_lib import datasets
import tensorflow.compat.v2 as tf

ds_test_keys = [('test_{}'.format(m), m) for m in datasets.ALL_DATASETS.keys()]


class DatasetTest(parameterized.TestCase):
  """Unit tests for datasets.py."""
  @parameterized.named_parameters(*ds_test_keys)
  def test_dataset_builder(self, ds):
    """Tests dataset builder."""
    num_shards = jax.local_device_count()
    batch_size = num_shards * 2
    eval_batch_size = num_shards * 1
    assert batch_size % num_shards == 0
    assert eval_batch_size % num_shards == 0

    dataset_builder = datasets.get_dataset(ds)
    dataset = dataset_builder(
        batch_size=batch_size,
        eval_batch_size=eval_batch_size,
        num_shards=num_shards)

    # a dataset should at least provide train_iter and valid_iter
    self.assertIsNotNone(dataset.train_iter)
    self.assertIsNotNone(dataset.valid_iter)

    train_batch = next(dataset.train_iter)
    eval_batch = next(dataset.valid_iter)

    # check inputs shapes
    expected_shape = jnp.array([
        num_shards, batch_size // num_shards,
        dataset.meta_data['input_shape'][1],
        dataset.meta_data['input_shape'][2], dataset.meta_data['input_shape'][3]
    ])
    expected_shape_eval = jnp.array([
        num_shards, eval_batch_size // num_shards,
        dataset.meta_data['input_shape'][1],
        dataset.meta_data['input_shape'][2], dataset.meta_data['input_shape'][3]
    ])

    self.assertTrue(
        jnp.array_equal(train_batch['inputs'].shape, expected_shape))
    self.assertTrue(
        jnp.array_equal(eval_batch['inputs'].shape, expected_shape_eval))

  @parameterized.named_parameters(*ds_test_keys)
  def test_dtypes(self, ds):
    """Tests data type of datasets."""
    num_shards = jax.local_device_count()
    batch_size = num_shards * 2
    eval_batch_size = num_shards * 1
    assert batch_size % num_shards == 0
    assert eval_batch_size % num_shards == 0

    for dt in dataset_utils.DATA_TYPE.keys():
      dataset_builder = datasets.get_dataset(ds)
      dataset = dataset_builder(
          batch_size=batch_size,
          eval_batch_size=eval_batch_size,
          num_shards=num_shards,
          dtype_str=dt)

      train_batch = next(dataset.train_iter)
      eval_batch = next(dataset.valid_iter)

      # check dtype
      self.assertEqual(train_batch['inputs'].dtype,
                       dataset_utils.DATA_TYPE[dt].jax_dtype)
      self.assertEqual(eval_batch['inputs'].dtype,
                       dataset_utils.DATA_TYPE[dt].jax_dtype)


if __name__ == '__main__':
  # enables eager execution: Necessary to use the TFDS loader
  tf.enable_v2_behavior()
  absltest.main()
