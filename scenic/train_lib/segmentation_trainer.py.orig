"""Training script for semantic segmentation tasks."""

import functools
import time
from typing import Any, Callable, Dict, Tuple, Optional, Type

from flax import jax_utils
import flax.linen as nn
import jax
from jax.experimental.optimizers import clip_grads
import jax.numpy as jnp
import ml_collections
import numpy as np
from scenic.dataset_lib import dataset_utils
from scenic.debug_lib import debug_utils
from scenic.model_lib.base_models import model_utils
from scenic.model_lib.base_models import segmentation_model
from scenic.train_lib import lr_schedules
from scenic.train_lib import optimizers
from scenic.train_lib import train_utils


# Aliases for custom types:
Batch = Dict[str, jnp.ndarray]
MetricFn = Callable[[jnp.ndarray, Dict[str, jnp.ndarray]],
                    Dict[str, Tuple[float, int]]]
LossFn = Callable[[jnp.ndarray, Batch, Optional[jnp.ndarray]], float]


def train_step(
    *,
    flax_model: nn.Module,
    train_state: train_utils.TrainState,
    batch: Batch,
    learning_rate_fn: Callable[[int], float],
    loss_fn: LossFn,
    metrics_fn: MetricFn,
    hparams: ml_collections.ConfigDict,
    debug: bool = False
) -> Tuple[train_utils.TrainState, Dict[str, Tuple[float, int]], float,
           jnp.ndarray]:
  """Runs a single step of training.

  Given the state of the training and a batch of data, computes
  the loss and updates the parameters of the model.

  Note that in this code, the buffers of the first (train_state) and second
  (batch) arguments are donated to the computation.

  Args:
    flax_model: A Flax model.
    train_state: The state of training including the current
      global_step, model_state, rng, and optimizer. The buffer of this argument
      can be donated to the computation.
    batch: A single batch of data. The buffer of this argument can be donated to
      the computation.
    learning_rate_fn: learning rate scheduler which give the global_step
      generates the learning rate.
    loss_fn: A loss function that given logits, a batch, and parameters of the
      model calculates the loss.
    metrics_fn: A metrics function that given logits and batch of data,
      calculates the metrics as well as the loss.
    hparams: Hyper-parameters for the experiment.
    debug: Whether the debug mode is enabled during training.
      `debug=True` enables model specific logging/storing some values using
      jax.host_callback.

  Returns:
    Updated state of training, computed metrics, learning rate, and predictions
      for logging.
  """
  new_rng, rng = jax.random.split(train_state.rng)
  # Bind the rng to the host/device we are on.
  dropout_rng = train_utils.bind_rng_to_host_device(
      rng, axis_name='batch', bind_to='device')

  def training_loss_fn(params):
    variables = {'params': params, **train_state.model_state}
    logits, new_model_state = flax_model.apply(
        variables,
        batch['inputs'],
        mutable=['batch_stats'],
        train=True,
        rngs={'dropout': dropout_rng},
        debug=debug)
    loss = loss_fn(logits, batch, variables['params'])
    return loss, (new_model_state, logits)

  compute_gradient_fn = jax.value_and_grad(training_loss_fn, has_aux=True)
  step = train_state.global_step
  lr = learning_rate_fn(step)
  (train_cost,
   (new_model_state,
    logits)), grad = compute_gradient_fn(train_state.optimizer.target)

  del train_cost
  # Re-use same axis_name as in the call to `pmap(...train_step...)` below.
  grad = jax.lax.pmean(grad, axis_name='batch')

  if hparams.get('max_grad_norm', None) is not None:
    grad = clip_grads(grad, hparams.max_grad_norm)

  new_optimizer = train_state.optimizer.apply_gradient(grad, learning_rate=lr)

  # Explicit weight decay, if necessary.
  if hparams.get('explicit_weight_decay', None) is not None:
    new_optimizer = new_optimizer.replace(
        target=optimizers.tree_map_with_names(
            functools.partial(
                optimizers.decay_weight_fn,
                lr=lr,
                decay=hparams.explicit_weight_decay),
            new_optimizer.target,
            match_name_fn=lambda name: 'kernel' in name))

  metrics = metrics_fn(logits, batch)
  new_train_state = train_state.replace(  # pytype: disable=attribute-error
      global_step=step + 1,
      optimizer=new_optimizer,
      model_state=new_model_state,
      rng=new_rng)
  return new_train_state, metrics, lr, jnp.argmax(logits, axis=-1)


def eval_step(
    *,
    flax_model: nn.Module,
    train_state: train_utils.TrainState,
    batch: Batch,
    metrics_fn: MetricFn,
    debug: bool = False
) -> Tuple[Batch, jnp.ndarray, Dict[str, Tuple[float, int]], jnp.ndarray]:
  """Runs a single step of training.

  Note that in this code, the buffer of the second argument (batch) is donated
  to the computation.

  Assumed API of metrics_fn is:
  ```metrics = metrics_fn(logits, batch)
  where batch is yielded by the batch iterator, and metrics is a dictionary
  mapping metric name to a vector of per example measurements. eval_step will
  aggregate (by summing) all per example measurements and divide by the
  aggregated normalizers. For each given metric we compute:
  1/N sum_{b in batch_iter} metric(b), where  N is the sum of normalizer
  over all batches.

  Args:
    flax_model: A Flax model.
    train_state: TrainState, the state of training including the current
      global_step, model_state, rng, and optimizer. The buffer of this argument
      can be donated to the computation.
    batch: A single batch of data. a metrics function, that given logits and
      batch of data, calculates the metrics as well as the loss.
    metrics_fn: A metrics function, that given logits and batch of data,
      calculates the metrics as well as the loss.
    debug: bool; Whether the debug mode is enabled during evaluation.
      `debug=True` enables model specific logging/storing some values using
      jax.host_callback.

  Returns:
    Batch, predictions and calculated metrics.
  """
  variables = {
      'params': train_state.optimizer.target,
      **train_state.model_state
  }
  logits = flax_model.apply(
      variables, batch['inputs'], train=False, mutable=False, debug=debug)
  metrics = metrics_fn(logits, batch)

  confusion_matrix = get_confusion_matrix(
      labels=batch['label'], logits=logits, batch_mask=batch['batch_mask'])

  # collect predictions and batches from all hosts
  predictions = jnp.argmax(logits, axis=-1)
  predictions = jax.lax.all_gather(predictions, 'batch')
  batch = jax.lax.all_gather(batch, 'batch')
  confusion_matrix = jax.lax.all_gather(confusion_matrix, 'batch')

  return batch, predictions, metrics, confusion_matrix


def get_confusion_matrix(*, labels, logits, batch_mask):
  """Computes the confusion matrix that is necessary for global mIoU."""
  if labels.ndim == logits.ndim:  # one-hot targets
    y_true = jnp.argmax(labels, axis=-1)
  else:
    y_true = labels
    # Set excluded pixels (label -1) to zero, because the confusion matrix
    # computation cannot deal with negative labels. They will be ignored due to
    # the batch_mask anyway:
    y_true = jnp.maximum(y_true, 0)
  y_pred = jnp.argmax(logits, axis=-1)

  # Prepare sample weights for confusion matrix:
  weights = batch_mask.astype(jnp.float32)
  # Normalize weights by number of samples to avoid having very large numbers in
  # the confusion matrix, which could lead to imprecise results (note that we
  # should not normalize by sum(weights) because that might differ between
  # devices/hosts):
  weights = weights / weights.size

  confusion_matrix = model_utils.confusion_matrix(
      y_true=y_true,
      y_pred=y_pred,
      num_classes=logits.shape[-1],
      weights=weights)
  confusion_matrix = confusion_matrix[jnp.newaxis, ...]  # Dummy batch dim.
  return confusion_matrix


def train(
    rng: np.ndarray, model_cls: Type[segmentation_model.SegmentationModel],
    dataset: dataset_utils.Dataset, hparams: ml_collections.ConfigDict,
    experiment_dir: str, summary_writer: Any
) -> Tuple[train_utils.TrainState, Dict[str, Any], Dict[str, Any]]:
  """Main training loop lives in this function.

  Given the model class and dataset, it prepares the items needed to run the
  training, including the TrainState.

  Args:
    rng: Jax rng key.
    model_cls: Model class; A model has a flax_model, a loss_fn, and a
      metrics_fn associated with it.
    dataset: The dataset that has train_iter, eval_iter, meta_data, and
      optionally, test_iter.
    hparams: Hyper-parameters for the experiment.
    experiment_dir: Directory in which we do checkpointing and write summary.
    summary_writer: Summary writer object.

  Returns:
    train_sate that has the state of training (including current
      global_step, model_state, rng, and the optimizer), train_summary
      and eval_summary which are dict of metrics. These outputs are used for
      regression testing.
  """
  master = jax.host_id() == 0
  # Build the loss_fn, metrics, and flax_model.
  model = model_cls(hparams, dataset.meta_data)

  # Initialize model.
  rng, init_rng = jax.random.split(rng)
  params, model_state, num_trainable_params = train_utils.initialize_model(
      model.flax_model, dataset.meta_data['input_shape'], hparams, init_rng,
      dataset.meta_data.get('input_dtype', jnp.float32))

  # Create optimizer.
  # We jit this, such that the a
  # rrays that are created are created on the same
  # device as the input is, in this case the CPU. Else they'd be on device[0].
  optimizer = jax.jit(
      optimizers.get_optimizer(hparams).create, backend='cpu')(
          params)
  rng, train_rng = jax.random.split(rng)
  train_state = train_utils.TrainState(
      global_step=0,
      optimizer=optimizer,
      model_state=model_state,
      rng=train_rng)
  start_step = train_state.global_step
  if hparams.checkpoint:
    train_state, start_step = train_utils.restore_checkpoint(
        experiment_dir, train_state)
  # Replicate the optimzier, state, and rng.
  train_state = jax_utils.replicate(train_state)
  del params  # Do not keep a copy of the initial params.

  # Calculate the total number of training steps.
  total_steps, steps_per_epoch = train_utils.get_num_training_steps(
      hparams, dataset.meta_data)
  # Get learning rate scheduler.
  learning_rate_fn = lr_schedules.get_learning_rate_fn(hparams)

  train_step_pmapped = jax.pmap(
      functools.partial(
          train_step,
          flax_model=model.flax_model,
          learning_rate_fn=learning_rate_fn,
          loss_fn=model.loss_function,
          metrics_fn=model.get_metrics_fn('train'),
          hparams=hparams,
          debug=hparams.debug_train),
      axis_name='batch',
      # We can donate both buffers of train_state and train_batch
      donate_argnums=(0, 1),
  )

  ############### EVALUATION CODE #################

  eval_step_pmapped = jax.pmap(
      functools.partial(
          eval_step,
          flax_model=model.flax_model,
          metrics_fn=model.get_metrics_fn('validation'),
          debug=hparams.debug_eval),
      axis_name='batch',
      # We can donate the eval_batch's buffer.
      donate_argnums=(1,),
  )

  # ceil rounding such that we include the last incomplete batch
  total_eval_steps = int(
      np.ceil(dataset.meta_data['num_eval_examples'] / hparams.batch_size))
  steps_per_eval = hparams.get('steps_per_eval') or total_eval_steps

  def evaluate(train_state: train_utils.TrainState,
               step: int) -> Dict[str, Any]:
    eval_metrics = []
    eval_all_confusion_mats = []
    # Sync model state across replicas.
    train_state = train_utils.sync_model_state_across_replicas(train_state)
    def to_cpu(x):
      return jax.device_get(dataset_utils.unshard(jax_utils.unreplicate(x)))
    for _ in range(steps_per_eval):
      eval_batch = next(dataset.valid_iter)
      e_batch, e_predictions, e_metrics, confusion_matrix = eval_step_pmapped(
          train_state=train_state, batch=eval_batch)
      eval_metrics.append(train_utils.unreplicate_and_get(e_metrics))
      # Evaluate global metrics on one of the hosts (master), but given
      # intermediate values collected from all hosts.
      if master and global_metrics_fn is not None:
        # Collect data to be sent for computing global metrics.
        eval_all_confusion_mats.append(to_cpu(confusion_matrix))

    eval_global_metrics_summary = {}
    if master and global_metrics_fn is not None:
      eval_global_metrics_summary = global_metrics_fn(eval_all_confusion_mats,
                                                      dataset.meta_data)

    ############### LOG EVAL SUMMARY ###############
    eval_summary = train_utils.log_eval_summary(
        step=step,
        eval_metrics=eval_metrics,
        extra_eval_summary=eval_global_metrics_summary,
        summary_writer=summary_writer)
    # Visualize val predictions for one batch:
    if master and summary_writer is not None:
      images = _draw_side_by_side(to_cpu(e_batch), to_cpu(e_predictions))
      for i, image in enumerate(images):
        summary_writer.image(f'val/example_{i}', image, step=step)
      summary_writer.flush()

    return eval_summary

  log_eval_steps = hparams.get('log_eval_steps') or steps_per_epoch
  if not log_eval_steps:
    raise ValueError("'log_eval_steps' should be specified in the config.")
  log_summary_steps = hparams.get('log_summary_steps') or log_eval_steps
  checkpoint_steps = hparams.get('checkpoint_steps') or log_eval_steps

  train_metrics, extra_training_logs = [], []
  tick = time.time()
  train_summary, eval_summary = None, None
  global_metrics_fn = model.get_global_metrics_fn()
  prof = None  # Keeps track of start/stop of profiler state.
  for step in range(start_step + 1, total_steps + 1):
    train_batch = next(dataset.train_iter)
    train_state, t_metrics, lr, train_predictions = train_step_pmapped(
        train_state=train_state, batch=train_batch)
    train_metrics.append(train_utils.unreplicate_and_get(t_metrics))

    # Additional training logs: time, learning rate, num parameters
    tock = time.time()
    train_logs = {
        'example_per_sec': np.asarray(1.0 / (tock - tick) * hparams.batch_size),
        'learning_rate': train_utils.unreplicate_and_get(lr),
        'num_trainable_params': np.asarray(num_trainable_params)
    }
    tick = tock
    extra_training_logs.append(train_logs)
    if step % log_summary_steps == 0:
      ############### LOG TRAIN SUMMARY ###############
      # Visualize segmentations using side-by-side gt-pred images:
      if master and summary_writer is not None:
        images = _draw_side_by_side(
            jax.device_get(dataset_utils.unshard(train_batch)),
            jax.device_get(dataset_utils.unshard(train_predictions)))
        for i, image in enumerate(images):
          summary_writer.image(f'train/example_{i}', image, step=step)

      train_summary = train_utils.log_train_summary(
          step=step,
          train_metrics=train_metrics,
          extra_training_logs=extra_training_logs,
          summary_writer=summary_writer)
      # reset metric accumulation for next evaluation cycle
      train_metrics, extra_training_logs = [], []

    if (step % log_eval_steps == 0) or (step == total_steps):
      # sync model state across replicas (in case of having model state, e.g.
      # batch statistic when using batch norm)
      train_state = train_utils.sync_model_state_across_replicas(train_state)
      eval_summary = evaluate(train_state, step)

    if ((step % checkpoint_steps == 0 and step > 0) or
        (step == total_steps)) and hparams.checkpoint:
      ################### CHECK POINTING ##########################
      # sync model state across replicas
      train_state = train_utils.sync_model_state_across_replicas(train_state)
      if master:
        train_utils.save_checkpoint(experiment_dir, train_state)

  # wait until computations are done before exiting
  jax.random.normal(jax.random.PRNGKey(0), ()).block_until_ready()
  # return the train and eval summary after last step for regresesion testing
  return train_state, train_summary, eval_summary


def _draw_side_by_side(batch, predictions):
  images = []
  for i in range(5):
    img = batch['inputs'][i]
    true = _apply_colormap(batch['label'][i])
    pred = _apply_colormap(predictions[i])
    images.append(np.concatenate([img, true, pred], axis=1))
  return images


def _apply_colormap(indexed):
  """Converts an array of color indices into an RGB image."""
  # This is Matplotlib's Tab20 color map:
  cmap = np.array([[0.12156863, 0.46666667, 0.70588235],
                   [0.68235294, 0.78039216, 0.90980392],
                   [1.00000000, 0.49803922, 0.05490196],
                   [1.00000000, 0.73333333, 0.47058824],
                   [0.17254902, 0.62745098, 0.17254902],
                   [0.59607843, 0.87450980, 0.54117647],
                   [0.83921569, 0.15294118, 0.15686275],
                   [1.00000000, 0.59607843, 0.58823529],
                   [0.58039216, 0.40392157, 0.74117647],
                   [0.77254902, 0.69019608, 0.83529412],
                   [0.54901961, 0.33725490, 0.29411765],
                   [0.76862745, 0.61176471, 0.58039216],
                   [0.89019608, 0.46666667, 0.76078431],
                   [0.96862745, 0.71372549, 0.82352941],
                   [0.49803922, 0.49803922, 0.49803922],
                   [0.78039216, 0.78039216, 0.78039216],
                   [0.73725490, 0.74117647, 0.13333333],
                   [0.85882353, 0.85882353, 0.55294118],
                   [0.09019608, 0.74509804, 0.81176471],
                   [0.61960784, 0.85490196, 0.89803922]])
  cind = (indexed % cmap.shape[0]).astype(int)
  img = cmap[cind]
  # Set excluded pixels to black:
  img = img * (indexed[..., np.newaxis] != -1)
  return img
