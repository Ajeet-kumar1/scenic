"""Training script for the DETR."""

import functools
import time

from absl import logging
from flax import jax_utils
from flax import nn
import jax
from jax.experimental.optimizers import clip_grads
import jax.numpy as jnp
import numpy as onp

import PIL
import PIL.ImageDraw
import PIL.ImageFont

from scenic.dataset_lib import dataset_utils
from scenic.debug_lib import debug_utils
from scenic.model_lib import models
from scenic.train_lib import lr_schedules
from scenic.train_lib import optimizers
from scenic.train_lib import set_detection_trainer
from scenic.train_lib import train_utils


def load_pretrianed_backbone(train_state, pretrained_backbone_hparams):
  """Loads a pretrained model for the backbone of detr.

  Args:
    train_state: TrainState, the state of training including the current
      global_step, model_state, rng, and optimizer. Here, it represents the
      initial state of training for DETR.
    pretrained_backbone_hparams: dict; hyper-parameters used for constructing
      the backbone model to load its checkpoint.

  Returns:
    train_state, in which the parameters and state of the backbone is replaced
    by parameters and state from a pretrained model.
  """
  # (1) load the train_state of the pretrained model from its checkpoint
  rng = jax.random.PRNGKey(0)
  model_cls = models.get_model_cls(pretrained_backbone_hparams.model_name)
  detr_model = model_cls(
      pretrained_backbone_hparams,
      pretrained_backbone_hparams.pretrained_dataset_meta_data)
  rng, model_rng = jax.random.split(rng)
  detr_module, model_state, _ = train_utils.create_flax_module(
      detr_model.flax_module_def,
      pretrained_backbone_hparams.pretrained_dataset_meta_data['input_shape'],
      pretrained_backbone_hparams, model_rng, jnp.float32)
  optimizer = jax.jit(
      optimizers.get_optimizer(pretrained_backbone_hparams).create,
      backend='cpu')(
          detr_module)
  rng, train_rng = jax.random.split(rng)
  pretrained_state = train_utils.TrainState(
      global_step=0,
      optimizer=optimizer,
      model_state=model_state,
      rng=train_rng)
  pretrained_state, _ = train_utils.restore_checkpoint(
      pretrained_backbone_hparams.checkpoint_dir, pretrained_state)

  # (2) replace the backbone params with pretrained params
  for m_key, m_params in pretrained_state.optimizer.target.params.items():
    # skip parameters related to the classifier head of the ResNet
    if m_key not in ['output_projection', 'pre_logits']:
      train_state.optimizer.target.params['backbone'][m_key] = m_params

  # (3) replace the backbone model_state (e.g. batch statistics)
  new_state_dict = {}
  for state_key, state_val in train_state.model_state.as_dict().items():
    if '/backbone' in state_key:
      new_state_dict[state_key] = pretrained_state.model_state[
          state_key.replace('/backbone', '')]
    else:
      new_state_dict[state_key] = state_val
  return train_state.replace(model_state=nn.Collection(new_state_dict))


def detr_train_forward_pass(train_state, batch):
  """Runs the forward pass of the model in train mode and get the outputs.

  Note that we don't update the train_state and basically don't change the RNG
  so for the next time we use the train_state to make predictions and compute
  the gradient, we will generate the same output.

  Args:
    train_state: TrainState; The state of training including the current
      global_step, model_state, rng, and optimizer. The buffer of this argument
      can be donated to the computation.
    batch: A single batch of data. The buffer of this argument can be donated to
      the computation.

  Returns:
    Predictions of the model.
  """
  _, rng = jax.random.split(train_state.rng)
  # bind the rng to the host/device we are on.
  model_rng = train_utils.bind_rng_to_host_device(
      rng, axis_name='batch', bind_to=['host', 'device'])

  # forward pass to get predictions (used for matching)
  with nn.stateful(train_state.model_state, mutable=False):
    with nn.stochastic(model_rng):
      predictions = train_state.optimizer.target(
          batch['inputs'], padding_mask=batch['padding_mask'], train=True,
          update_batch_stats=False, debug=False)

  return predictions


def detr_train_step(train_state,
                    batch,
                    matches,
                    learning_rate_fn,
                    loss_and_metrics_fn,
                    max_grad_norm=None,
                    update_model_state=True,
                    debug=False):
  """Runs a single step of training.

  Given the state of the training and a batch of data, computes
  the loss and updates the parameters of the model.

  Note that in this code, the buffers of the first (train_state) and second
  (batch) arguments are donated to the computation.

  Args:
    train_state: TrainState; The state of training including the current
      global_step, model_state, rng, and optimizer. The buffer of this argument
      can be donated to the computation.
    batch: A single batch of data. The buffer of this argument can be donated to
      the computation.
    matches: nd-array: alignment of predictions with targets to be used for
      computing the loss.
    learning_rate_fn: learning rate scheduler which give the global_step
      generates the learning rate.
    loss_and_metrics_fn: A function that given model predictions, a batch, and
      parameters of the model calculates the loss as well as metrics.
    max_grad_norm: float; Maximum gradient norm used for gradient clipping. If
      set to None, no gradient clipping happens.
    update_model_state: bool; whether to update the model_state (e.g. batch
      stats in BatchNorm) during training or freeze it.
    debug: bool; Whether the debug mode is enabled during training. `debug=True`
      enables model specific logging/storing some values using
      jax.host_callback.

  Returns:
    Updated state of training and calculated metrics and the learning rate. Also
    gradients from this step.

  """
  # TODO(dehghani): add multi-optimizer
  new_rng, rng = jax.random.split(train_state.rng)
  # bind the rng to the host/device we are on.
  model_rng = train_utils.bind_rng_to_host_device(
      rng, axis_name='batch', bind_to=['host', 'device'])

  if update_model_state:

    def training_loss_fn(detr_module):
      with nn.stateful(train_state.model_state) as new_model_state:
        with nn.stochastic(model_rng):
          predictions = detr_module(
              batch['inputs'], padding_mask=batch['padding_mask'], train=True,
              update_batch_stats=True, debug=debug)
      loss, metrics = loss_and_metrics_fn(
          predictions, batch, matches=matches, model_params=detr_module.params)
      return loss, (new_model_state, metrics)

  else:  # freeze the model state (e.g. batch statistics)

    def training_loss_fn(detr_module):
      with nn.stateful(train_state.model_state, mutable=False):
        with nn.stochastic(model_rng):
          predictions = detr_module(
              batch['inputs'],
              padding_mask=batch['padding_mask'],
              train=True,
              update_batch_stats=False,
              debug=debug)
      loss, metrics = loss_and_metrics_fn(
          predictions, batch, matches=matches, model_params=detr_module.params)
      return loss, (train_state.model_state, metrics)

  compute_gradient_fn = jax.value_and_grad(training_loss_fn, has_aux=True)
  step = train_state.global_step
  lr = learning_rate_fn(step)
  (train_cost,
   (new_model_state,
    metrics)), grad = compute_gradient_fn(train_state.optimizer.target)

  if max_grad_norm is not None:
    grad = clip_grads(grad, max_grad_norm)

  del train_cost
  # re-use same axis_name as in the call to `pmap(...train_step...)` below
  grad = jax.lax.pmean(grad, axis_name='batch')
  new_optimizer = train_state.optimizer.apply_gradient(grad, learning_rate=lr)
  new_train_state = train_state.replace(
      global_step=step + 1,
      optimizer=new_optimizer,
      model_state=new_model_state,
      rng=new_rng)
  return new_train_state, metrics, lr, grad


def train(rng, model_cls, dataset, hparams, experiment_dir, summary_writer):
  """Main training loop lives in this function.

  Given the model class and dataset, it prepares the items needed to run the
  training, including the TrainState.

  Args:
    rng: Jax rng key.
    model_cls: Model class; A model has a flax_module, a loss_and_metrics_fn,
      and a associated with it.
    dataset: The dataset that has train_iter, eval_iter, meta_data, and
      optionally, test_iter.
    hparams: hyper-parameters for the experiment.
    experiment_dir: Directory in which we do checkpointing and write summary.
    summary_writer: Summary writer object.

  Returns:
    train_sate that has the state of training (including current
      global_step, model_state, rng, and the optimizer), train_summary
      and eval_summary which are dict of metrics. These outputs are used for
      regression testing.
  """
  master = jax.host_id() == 0
  # build the loss_and_metrics_fn, metrics, and flax_module
  detr_model = model_cls(hparams, dataset.meta_data)
  # create flax module
  rng, model_rng = jax.random.split(rng)
  detr_module, model_state, num_trainable_params = train_utils.create_flax_module(
      detr_model.flax_module_def, dataset.meta_data['input_shape'], hparams,
      model_rng, dataset.meta_data.get('input_dtype', jnp.float32))

  # create optimizer
  # we jit this, such that the arrays that are created are created on the same
  # device as the input is, in this case the CPU. Else they'd be on device[0]
  optimizer = jax.jit(
      optimizers.get_optimizer(hparams).create, backend='cpu')(
          detr_module)
  rng, train_rng = jax.random.split(rng)
  train_state = train_utils.TrainState(
      global_step=0,
      optimizer=optimizer,
      model_state=model_state,
      rng=train_rng)
  start_step = train_state.global_step
  if hparams.checkpoint:
    train_state, start_step = train_utils.restore_checkpoint(
        experiment_dir, train_state)

  if start_step == 0 and hparams.get('load_pretrained_backbone', False):
    # only load pretrained backbone if we are at the beginning of training
    train_state = load_pretrianed_backbone(train_state,
                                           hparams.pretrained_backbone_configs)

  update_model_state = not hparams.get('freeze_backbone_batch_stats', False)
  if not update_model_state:
    if not hparams.load_pretrained_backbone:
      raise ValueError('Freezing the batch statistics of the resnet backbone '
                       'is only possible when loading a pretrained resnet '
                       'backbone is enabled.')
  # replicate the optimzier, state, and rng
  train_state = jax_utils.replicate(train_state)
  del detr_module  # do not keep a copy of the initial model

  # calculate the total number of training steps
  total_steps, steps_per_epoch = train_utils.get_num_training_steps(
      hparams, dataset.meta_data)
  # get learning rate scheduler
  learning_rate_fn = lr_schedules.get_learning_rate_fn(hparams)

  train_forward_pass_pmapped = jax.pmap(
      detr_train_forward_pass, axis_name='batch')
  train_step_pmapped = jax.pmap(
      functools.partial(
          detr_train_step,
          learning_rate_fn=learning_rate_fn,
          loss_and_metrics_fn=detr_model.loss_function,
          update_model_state=update_model_state,
          max_grad_norm=hparams.get('max_grad_norm', None),
          debug=hparams.debug_train),
      axis_name='batch',
      # we can donate both buffers of train_state and train_batch
      donate_argnums=(0, 1),
  )
  eval_forward_pass_pmapped = jax.pmap(
      set_detection_trainer.eval_forward_pass, axis_name='batch')
  eval_step_pmapped = jax.pmap(
      functools.partial(
          set_detection_trainer.eval_step,
          loss_and_metrics_fn=detr_model.loss_function,
          debug=hparams.debug_eval),
      axis_name='batch',
      # we can donate the eval_batch's buffer
      donate_argnums=(1,),
  )
  eval_frequency = hparams.get('eval_frequency') or steps_per_epoch
  if not eval_frequency:
    raise ValueError("'eval_frequency' should be specified in the config.")
  summary_frequency = hparams.get('summary_frequency', 25)
  checkpoint_frequency = hparams.get('checkpoint_frequency') or eval_frequency
  # ceil rounding such that we include the last incomplete batch
  total_eval_steps = int(
      onp.ceil(dataset.meta_data['num_eval_examples'] / hparams.batch_size))
  steps_per_eval = hparams.get('steps_per_eval') or total_eval_steps

  global_metrics_fn = detr_model.get_global_metrics_fn()
  train_metrics, extra_training_logs = [], []
  tick = time.time()
  train_summary, eval_summary, eval_global_metrics_summary = None, None, None
  for step in range(start_step + 1, total_steps + 1):
    train_batch = next(dataset.train_iter)
    # get predictions to create the matches
    train_predictions = train_forward_pass_pmapped(train_state, train_batch)
    # do the matching on CPU
    train_matches = set_detection_trainer.get_matching(train_predictions,
                                                       train_batch,
                                                       detr_model.matcher)
    # do the train step given the matches
    train_state, t_metrics, lr, grad = train_step_pmapped(
        train_state, train_batch, matches=train_matches)
    train_metrics.append(t_metrics)

    # additional training logs: time, learning rate, num parameters
    tock = time.time()
    train_logs = {
        'example_per_sec': 1.0 / onp.asarray(tock - tick) * hparams.batch_size,
        'learning_rate': onp.asarray(jax_utils.unreplicate(lr)),
        'num_trainable_params': onp.asarray(num_trainable_params)
    }
    tick = tock
    extra_training_logs.append(train_logs)

    metrics_normalizer_fn = functools.partial(
        set_detection_trainer.normalize_metrics_summary,
        object_detection_loss_keys=detr_model.loss_terms_weights.keys())

    if (step % summary_frequency == 0) or (step == total_steps):
      ############### LOG TRAIN SUMMARY ###############
      # dumps gradient histogram summaries
      if master and summary_writer is not None:
        grad_np = jax.device_get(jax_utils.unreplicate(grad))
        grad_params, _ = jax.tree_flatten(grad_np.params)
        for i, g in enumerate(grad_params):
          summary_writer.histogram(f'grad_{i}', g, step)

      # visualizes detections using side-by-side gt-pred images
      if master and summary_writer is not None:
        viz = draw_side_by_side(
            jax.device_get(dataset_utils.unshard(train_predictions)),
            jax.device_get(dataset_utils.unshard(train_batch)))
        for i, viz_ in enumerate(viz):
          summary_writer.image(f'sidebyside_{i}', viz_, step=step)

      train_summary = train_utils.log_train_summary(
          step=step,
          train_metrics=train_metrics,
          extra_training_logs=extra_training_logs,
          summary_writer=summary_writer,
          metrics_normalizer_fn=metrics_normalizer_fn)
      # reset metric accumulation for next evaluation cycle
      train_metrics, extra_training_logs = [], []
      #################################################

    if (step % eval_frequency == 0) or (step == total_steps):
      # this section is for running evaluation on the validation set. Note that
      # this is not working for the multi-host setup
      if jax.host_count() > 1:
        # TODO(dehghani): lift this constraint
        logging.warning('Testing on whole data will give incorrect results.')

      eval_metrics = []
      eval_all_predictions = []
      eval_all_labels = []
      # sync model state across replicas (in case of having model state, e.g.
      # batch statistic when using batch norm)
      train_state = train_utils.sync_model_state_across_replicas(train_state)
      for _ in range(steps_per_eval):
        eval_batch = next(dataset.valid_iter)
        # get predictions to create the matches
        eval_predictions = eval_forward_pass_pmapped(train_state, eval_batch)
        # do the matching on CPU
        eval_matches = set_detection_trainer.get_matching(
            eval_predictions, eval_batch, detr_model.matcher)
        # do the eval step given the matches
        e_predictions, e_metrics = eval_step_pmapped(
            train_state, eval_batch, matches=eval_matches)
        e_predictions.pop('aux_outputs', None)  # Not needed anymore.
        # collect local metrics (returned by the loss function)
        eval_metrics.append(e_metrics)

        # collect preds and labels to be sent for computing global metrics
        e_pred_list = set_detection_trainer.process_and_fetch_to_host(
            e_predictions)
        e_lbls_list = set_detection_trainer.process_and_fetch_to_host(
            eval_batch['label'])
        # reshape from [n_dev, bs, ...] to [host_bs, ...]
        e_weight = eval_batch['batch_mask'].reshape(
            (-1,) + eval_batch['batch_mask'].shape[2:])
        if eval_batch.get('batch_mask', None) is not None:
          e_pred_list, e_lbls_list = set_detection_trainer.filter_pred_or_tgt_list(
              e_weight, e_pred_list, e_lbls_list)
        eval_all_predictions.extend(e_pred_list)
        eval_all_labels.extend(e_lbls_list)

      expected_num_eval_examples = (
          hparams.get('steps_per_eval') *
          hparams.batch_size if hparams.get('steps_per_eval') is not None else
          dataset.meta_data['num_eval_examples'])
      npred = len(eval_all_predictions)
      nlbl = len(eval_all_labels)
      if jax.host_count() == 1:
        assert (npred == nlbl == expected_num_eval_examples), (
            f'{npred} == {nlbl} == {expected_num_eval_examples}?')
      else:
        assert npred == nlbl, f'{npred} == {nlbl}?'

      eval_global_metrics_summary = global_metrics_fn(eval_all_predictions,
                                                      eval_all_labels)
      ############### LOG EVAL SUMMARY ###############
      eval_summary = train_utils.log_eval_summary(
          step=step,
          eval_metrics=eval_metrics,
          extra_eval_summary=eval_global_metrics_summary,
          summary_writer=summary_writer,
          metrics_normalizer_fn=metrics_normalizer_fn)

    ##################### CHECK POINTING ############################
    if ((step % checkpoint_frequency == 0 and step > 0) or
        (step == total_steps)) and hparams.checkpoint:
      # sync model state across replicas
      train_state = train_utils.sync_model_state_across_replicas(train_state)
      if master:
        train_utils.save_checkpoint(experiment_dir, train_state)

  # wait until computations are done before exiting
  jax.random.normal(jax.random.PRNGKey(0), ()).block_until_ready()
  # return the train and eval summary after last step for regresesion testing
  eval_summary.update(eval_global_metrics_summary)
  return train_state, train_summary, eval_summary


def draw_side_by_side(pred, batch):
  """Side-by-side visualization of model predictions and ground truth."""

  viz = []

  # unnormalizes images to be [0, 1]
  mean_rgb = onp.reshape(onp.array([0.48, 0.456, 0.406]), [1, 1, 1, 3])
  std_rgb = onp.reshape(onp.array([0.229, 0.224, 0.225]), [1, 1, 1, 3])
  imgs = ((batch['inputs'] * std_rgb + mean_rgb) * 255.0).astype(onp.uint8)

  font = PIL.ImageFont.load_default()

  # iterates over images in the batch and makes visualizations
  for indx in range(imgs.shape[0]):
    h, w = batch['label']['size'][indx]

    # first for ground truth
    gtim = PIL.Image.fromarray(imgs[indx])
    gtdraw = PIL.ImageDraw.Draw(gtim)
    for bb, cls, is_crowd in zip(batch['label']['boxes'][indx],
                                 batch['label']['labels'][indx],
                                 batch['label']['is_crowd'][indx]):
      if cls == 0:
        continue  # dummy object.

      bcx, bcy, bw, bh = bb * [w, h, w, h]
      bb = [bcx - bw/2, bcy - bh/2, bcx + bw/2, bcy + bh/2]
      if is_crowd:
        edgecolor = (255, 0, 0)
      else:
        edgecolor = (255, 255, 0)

      gtdraw.rectangle(bb, fill=None, outline=edgecolor, width=3)
      gtdraw.text([bb[0], max(bb[1]-10, 0)], COCO_LABEL_MAP[cls],
                  font=font, fill=(0, 0, 255))

    # second for model predictions
    predim = PIL.Image.fromarray(imgs[indx])
    preddraw = PIL.ImageDraw.Draw(predim)
    pred_lbls = onp.argmax(pred['pred_logits'], axis=-1)
    for bb, cls in zip(pred['pred_boxes'][indx], pred_lbls[indx]):
      h, w = batch['label']['size'][indx]
      bcx, bcy, bw, bh = bb * [w, h, w, h]
      bb = [bcx - bw/2, bcy - bh/2, bcx + bw/2, bcy + bh/2]
      edgecolor = (0, 255, 0)
      preddraw.rectangle(bb, fill=None, outline=edgecolor, width=3)
      preddraw.text([bb[0], max(bb[1]-10, 0)], COCO_LABEL_MAP[cls],
                    font=font, fill=(0, 0, 255))

    gtim_np = onp.asarray(gtim)
    predim_np = onp.asarray(predim)
    composite = onp.concatenate([predim_np, gtim_np], axis=1)

    viz.append(composite)
  return onp.stack(viz, axis=0)


COCO_LABEL_MAP = {
    0: 'padding',
    1: 'person',
    2: 'bicycle',
    3: 'car',
    4: 'motorcycle',
    5: 'airplane',
    6: 'bus',
    7: 'train',
    8: 'truck',
    9: 'boat',
    10: 'traffic light',
    11: 'fire hydrant',
    12: 'stop sign',
    13: 'parking meter',
    14: 'bench',
    15: 'bird',
    16: 'cat',
    17: 'dog',
    18: 'horse',
    19: 'sheep',
    20: 'cow',
    21: 'elephant',
    22: 'bear',
    23: 'zebra',
    24: 'giraffe',
    25: 'backpack',
    26: 'umbrella',
    27: 'handbag',
    28: 'tie',
    29: 'suitcase',
    30: 'frisbee',
    31: 'skis',
    32: 'snowboard',
    33: 'sports ball',
    34: 'kite',
    35: 'baseball bat',
    36: 'baseball glove',
    37: 'skateboard',
    38: 'surfboard',
    39: 'tennis racket',
    40: 'bottle',
    41: 'wine glass',
    42: 'cup',
    43: 'fork',
    44: 'knife',
    45: 'spoon',
    46: 'bowl',
    47: 'banana',
    48: 'apple',
    49: 'sandwich',
    50: 'orange',
    51: 'broccoli',
    52: 'carrot',
    53: 'hot dog',
    54: 'pizza',
    55: 'donut',
    56: 'cake',
    57: 'chair',
    58: 'couch',
    59: 'potted plant',
    60: 'bed',
    61: 'dining table',
    62: 'toilet',
    63: 'tv',
    64: 'laptop',
    65: 'mouse',
    66: 'remote',
    67: 'keyboard',
    68: 'cell phone',
    69: 'microwave',
    70: 'oven',
    71: 'toaster',
    72: 'sink',
    73: 'refrigerator',
    74: 'book',
    75: 'clock',
    76: 'vase',
    77: 'scissors',
    78: 'teddy bear',
    79: 'hair drier',
    80: 'toothbrush',
}
