# Lint as: python3
"""Vision Transformer."""

from absl import logging
import flax.linen as nn
import jax.numpy as jnp
import ml_collections
import numpy as np
from scenic.model_lib.base_models import model_utils as base_model_utils
from scenic.model_lib.base_models.multilabel_classification_model import MultiLabelClassificationModel
from scenic.model_lib.layers import nn_layers
import scipy


class AddPositionEmbs(nn.Module):
  """Adds (optionally learned) positional embeddings to the inputs."""

  def apply(self, inputs, posemb_init=None):
    """Applies AddPositionEmbs module.

    By default this layer uses a fixed sinusoidal embedding table. If a
    learned position embedding is desired, pass an initializer to
    posemb_init.

    Args:
      inputs: nd-array; Input data.
      posemb_init: Positional embedding initializer.

    Returns:
      Output in shape `[bs, timesteps, in_dim]`
    """
    # inputs.shape is (batch_size, seq_len, emb_dim).
    assert inputs.ndim == 3, ('Number of dimensions should be 3,'
                              ' but it is: %d' % inputs.ndim)
    pos_emb_shape = (1, inputs.shape[1], inputs.shape[2])
    pe = self.param('pos_embedding', pos_emb_shape, posemb_init)
    return inputs + pe


class MlpBlock(nn.Module):
  """Transformer MLP / feed-forward block."""

  def apply(self,
            inputs,
            mlp_dim,
            dtype=jnp.float32,
            out_dim=None,
            dropout_rate=0.1,
            deterministic=True,
            kernel_init=nn.initializers.xavier_uniform(),
            bias_init=nn.initializers.normal(stddev=1e-6)):
    """Applies Transformer MlpBlock module."""
    actual_out_dim = inputs.shape[-1] if out_dim is None else out_dim
    x = nn.Dense(
        inputs,
        mlp_dim,
        dtype=dtype,
        kernel_init=kernel_init,
        bias_init=bias_init)
    x = nn.gelu(x)
    x = nn.dropout(x, rate=dropout_rate, deterministic=deterministic)
    output = nn.Dense(
        x,
        actual_out_dim,
        dtype=dtype,
        kernel_init=kernel_init,
        bias_init=bias_init)
    output = nn.dropout(output, rate=dropout_rate, deterministic=deterministic)
    return output


class Encoder1DBlock(nn.Module):
  """Transformer encoder layer."""

  def apply(self,
            inputs,
            mlp_dim,
            num_heads,
            dropout_rate=0.1,
            attention_dropout_rate=0.1,
            deterministic=True,
            dtype=jnp.float32):
    """Applies Encoder1DBlock module.

    Args:
      inputs: nd-array; Input data.
      mlp_dim: int; Dimension of the mlp on top of attention block.
      num_heads: int; Number of heads.
      dropout_rate: float; Dropout rate.
      attention_dropout_rate: float; Dropout for attention heads.
      deterministic: bool, Deterministic or not (to apply dropout).
      dtype: the dtype of the computation (default: float32).

    Returns:
      Output after transformer encoder block.
    """

    # Attention block.
    assert inputs.ndim == 3
    x = nn.LayerNorm(inputs, dtype=dtype)
    x = nn.SelfAttention(
        x,
        num_heads=num_heads,
        inputs_kv=x,
        attention_axis=(1,),
        causal_mask=False,
        kernel_init=nn.initializers.xavier_uniform(),
        broadcast_dropout=False,
        deterministic=deterministic,
        dropout_rate=attention_dropout_rate,
        dtype=dtype)
    x = nn.dropout(x, rate=dropout_rate, deterministic=deterministic)
    x = x + inputs

    # MLP block.
    y = nn.LayerNorm(x, dtype=dtype)
    y = MlpBlock(
        y,
        mlp_dim=mlp_dim,
        dtype=dtype,
        dropout_rate=dropout_rate,
        deterministic=deterministic)

    return x + y


class Encoder(nn.Module):
  """Transformer Encoder."""

  def apply(self,
            inputs,
            mlp_dim,
            num_layers,
            num_heads,
            dropout_rate=0.1,
            attention_dropout_rate=0.1,
            train=False):
    """Applies Transformer model on the inputs.

    Args:
      inputs: nd-array, Input data
      mlp_dim: int; Dimension of the mlp on top of attention block.
      num_layers: int; Number of layers.
      num_heads: int; Number of attention heads.
      dropout_rate: float; Dropout rate.
      attention_dropout_rate: float; Dropout for attention heads.
      train: bool; If it is training,

    Returns:
      Output of a transformer encoder.
    """
    assert inputs.ndim == 3  # (batch, len, emb)

    x = AddPositionEmbs(
        inputs,
        posemb_init=nn.initializers.normal(stddev=0.02),  # from BERT.
        name='posembed_input')
    x = nn.dropout(x, rate=dropout_rate, deterministic=not train)

    # Input Encoder
    for lyr in range(num_layers):
      x = Encoder1DBlock(
          x,
          mlp_dim=mlp_dim,
          num_heads=num_heads,
          dropout_rate=dropout_rate,
          attention_dropout_rate=attention_dropout_rate,
          deterministic=not train,
          name=f'encoderblock_{lyr}')

    encoded = nn.LayerNorm(x, name='encoder_norm')

    return encoded


class ViT(nn.Module):
  """Vision Transformer model."""

  def apply(self,
            x,
            mlp_dim,
            num_layers,
            num_heads,
            num_classes,
            patches,
            hidden_size,
            representation_size=None,
            dropout_rate=0.1,
            attention_dropout_rate=0.1,
            classifier='gap',
            train=True,
            debug=False):

    n, h, w, c = x.shape
    if patches.get('grid') is not None:
      assert patches.get('size') is None, ('You should either set the '
                                           'patches.grid or patches.size.')
      gh, gw = patches.grid
      fh, fw = h // gh, w // gw
    else:
      fh, fw = patches.size
      gh, gw = h // fh, w // fw
    if hidden_size:
      x = nn.Conv(
          x,
          hidden_size, (fh, fw),
          strides=(fh, fw),
          padding='VALID',
          name='embedding')
    else:
      # This path often results in excessive padding: b/165788633
      x = jnp.reshape(x, [n, gh, fh, gw, fw, c])
      x = jnp.transpose(x, [0, 1, 3, 2, 4, 5])
      x = jnp.reshape(x, [n, gh, gw, -1])

    n, h, w, c = x.shape
    x = jnp.reshape(x, [n, h * w, c])

    # If we want to add a class token, add it here.
    if classifier == 'token':
      cls = self.param('cls', (1, 1, c), nn.initializers.zeros)
      cls = jnp.tile(cls, [n, 1, 1])
      x = jnp.concatenate([cls, x], axis=1)

    x = Encoder(
        x,
        mlp_dim=mlp_dim,
        num_layers=num_layers,
        num_heads=num_heads,
        dropout_rate=dropout_rate,
        attention_dropout_rate=attention_dropout_rate,
        name='Transformer',
        train=train)

    if classifier == 'token':
      x = x[:, 0]
    elif classifier == 'gap':
      x = jnp.mean(x, axis=list(range(1, x.ndim - 1)))

    if representation_size is not None:
      x = nn.Dense(x, representation_size, name='pre_logits')
      x = nn.tanh(x)
    else:
      x = nn_layers.IdentityLayer(x, name='pre_logits')

    x = nn.Dense(
        x,
        num_classes,
        kernel_init=nn.initializers.zeros,
        name='output_projection')
    return x


class ViTMultiLabelClassificationModel(MultiLabelClassificationModel):
  """Vistion Transformer model for multi-label classification task."""

  def build_flax_module(self):
    return ViT.partial(
        num_classes=self.dataset_meta_data['num_classes'],
        mlp_dim=self.hparams.model.mlp_dim,
        num_layers=self.hparams.model.num_layers,
        num_heads=self.hparams.model.num_heads,
        representation_size=self.hparams.model.representation_size,
        patches=self.hparams.model.patches,
        hidden_size=self.hparams.model.hidden_size,
        classifier=self.hparams.model.classifier,
        dropout_rate=self.hparams.model.get('dropout_rate', 0.1),
        attention_dropout_rate=self.hparams.model.get('attention_dropout_rate',
                                                      0.1),
    )

  def default_flax_module_hparams(self):
    return ml_collections.ConfigDict({
        'model':
            dict(
                num_heads=2,
                num_layers=1,
                representation_size=16,
                mlp_dim=32,
                dropout_rate=0.,
                attention_dropout_rate=0.,
                hidden_size=16,
                patches={'grid': (4, 4)},
                classifier='gap',
                data_dtype_str='float32')
    })

  def init_from_train_state(self, train_state, restored_train_state,
                            restored_model_cfg):
    """Updates the train_state with data from restored_train_state.

    This function is writen to be used for 'fine-tuning' experiments. Here, we
    do some surgery to support larger resolutions (longer sequence length) in
    the transformer block, with respect to the learned pos-embeddings.


    Args:
      train_state: A raw TrainState for the model.
      restored_train_state: A TrainState that is loaded with parameters/state of
        a  pretrained model.
      restored_model_cfg: Configuration of the model from which the
        restored_train_state come from. Usually used for some asserts.

    Returns:
      Updated train_state.
    """
    # inspect and compare the parameters of the model with the init-model
    restored_params = base_model_utils.inspect_params(
        restored_params=restored_train_state.optimizer['target']['params'],
        expected_params=train_state.optimizer.target.params,
        fail_if_extra=False,
        fail_if_missing=False)

    # start moving parameters, one-by-one and apply changes if needed
    for m_key, m_params in restored_params.items():
      if m_key == 'output_projection':
        # for the classifier head, we use a the randomly initialized params and
        #   ignore the the one from pretrained model
        pass

      elif m_key == 'pre_logits':
        if self.hparams.model.representation_size is None:
          # we don't have representation_size in the new model, so lets ignore
          #   if from the pretained model, in case it has it.
          train_state.optimizer.target.params[m_key] = {}
        else:
          assert restored_model_cfg.model.representation_size
          train_state.optimizer.target.params[m_key] = m_params

      elif m_key == 'Transformer':
        for tm_key, tm_params in m_params.items():
          if tm_key == 'posembed_input':  # might need resolution change
            posemb = train_state.optimizer.target.params[m_key][
                'posembed_input']['pos_embedding']
            restored_posemb = m_params['posembed_input']['pos_embedding']

            if restored_posemb.shape != posemb.shape:
              # rescale the grid of pos, embeddings: param shape is (1, N, 768)
              logging.info('Resized variant: %s to %s', restored_posemb.shape,
                           posemb.shape)
              ntok = posemb.shape[1]
              if restored_model_cfg.model.classifier == 'token':
                # the first token is the CLS token
                cls_tok = restored_posemb[:, :1]
                restored_posemb_grid = restored_posemb[0, 1:]
                ntok -= 1
              else:
                cls_tok = restored_posemb[:, :0]
                restored_posemb_grid = restored_posemb[0]

              restored_gs = int(np.sqrt(len(restored_posemb_grid)))
              gs = int(np.sqrt(ntok))
              if restored_gs != gs:  # we need resolution change
                logging.info('Grid-size from %s to %s.', restored_gs, gs)
                restored_posemb_grid = restored_posemb_grid.reshape(
                    restored_gs, restored_gs, -1)
                zoom = (gs / restored_gs, gs / restored_gs, 1)
                restored_posemb_grid = scipy.ndimage.zoom(
                    restored_posemb_grid, zoom, order=1)
                restored_posemb_grid = restored_posemb_grid.reshape(
                    1, gs * gs, -1)
                # attache the CLS token again
                restored_posemb = jnp.array(
                    np.concatenate([cls_tok, restored_posemb_grid], axis=1))

            train_state.optimizer.target.params[m_key][tm_key][
                'pos_embedding'] = restored_posemb
          else:  # other parameters of the Transformer encoder
            train_state.optimizer.target.params[m_key][tm_key] = tm_params

      else:
        # use the rest as they are in the pretrianed model
        train_state.optimizer.target.params[m_key] = m_params

    return train_state
