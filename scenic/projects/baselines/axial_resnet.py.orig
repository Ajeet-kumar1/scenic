"""Implementation of AxialResNet with group norm and weight standartization.

Ported from:
https://arxiv.org/abs/2003.07853
based on:
https://github.com/csrhddlam/axial-deeplab/tree/optimize
"""

from typing import Tuple

import flax.linen as nn
import jax
import jax.numpy as jnp
import ml_collections
from scenic.model_lib.layers import nn_ops
from scenic.projects.baselines import bit_resnet


class SelfAttentionWith1DRelativePos(nn.Module):
  """Multi-head dot-product self-attention with reltive positional encodeing.

  Attributes:
    num_heads: Number of attention heads.
  """

  num_heads: int

  @nn.compact
  def __call__(self, x: jnp.array) -> jnp.ndarray:
    """Applies multi-head dot product self-attention on the input data.

    Args:
      x: input of shape `[bs, len, features]`.

    Returns:
      output of shape `[bs, len, features]`.
    """
    bs, input_len, features = x.shape
    if features % (2 * self.num_heads) != 0:
      raise ValueError(
          f'Inputs feature dimension {features} must be divisible by 2 * number '
          f'of heads {2*self.num_heads}.')
    head_features = features // self.num_heads

    # Apply qkv transformation.
    qkv = nn.Dense(
        features=features * 2,
        kernel_init=nn.initializers.normal(stddev=1.0 / features),
        use_bias=False,
        dtype=jnp.float32)(
            x)

    # Normalize.
    qkv = nn.GroupNorm(epsilon=1e-4, name='gn_qkv')(qkv)
    # Split to multi-headed.
    qkv = qkv.reshape(bs, input_len, self.num_heads,
                      head_features * 2).transpose((0, 2, 3, 1))  # To bhdl.
    # Following the reference implementation, we set feature-size per head
    #  of query, key, to half of the feature size per head for value:
    query, key, value = jnp.split(
        qkv, [head_features // 2, head_features], axis=2)

    # Compute relative positional attention logits.
    length = query.shape[-1]  # Shape: `[bs, heads, depth, len]`.
    relative_emb = self.param(
        'relative_pos_emb', nn.initializers.normal(stddev=1.0 / head_features),
        (head_features * 2, 2 * length - 1), jnp.float32)
    relative_pos_emb = jnp.take(
        relative_emb,
        nn_ops.compute_1d_relative_distance(length, length),
        axis=-1)
    relative_pos_emb_q, relative_pos_emb_k, relative_pos_emb_v = jnp.split(
        relative_pos_emb, [head_features // 2, head_features], axis=0)

    # When computing the similarity of keys and queries, we in fact have
    # (Q + P_q)*(K + P_k), which is Q*K + K*P_q + Q*P_k + P_q*P_k.
    # The last term, i.e. P_q*P_k, is fixed for all Q, K, but but we compute
    # the attention logits for other three terms:
    # 1) We attend from content of queries to the relative position of keys,
    # i.e. Q*P_k:
    qr_attn_logits = jnp.einsum('bhdi,dij->bhij', query, relative_pos_emb_k)
    # 2) We attend from content of keys to the relative position of queries,
    # i.e. K*P_q
    kr_attn_logits = jnp.einsum('bhdi,dij->bhij', key, relative_pos_emb_q)
    # 3) We attend from content of queries to the content of keys, i.e. Q*K:
    qk_attn_logits = jnp.einsum('bhdi,bhdj->bhij', query, key)
    # Finally we combine all these attention logits:
    attn_weights = qk_attn_logits + qr_attn_logits + kr_attn_logits

    # Normalize the attention weights with softmax.
    attn_weights = jax.nn.softmax(attn_weights, axis=3).astype(jnp.float32)
    # Weighted sum over values for each query position.
    wv = jnp.einsum('bhij,bhdj->bhdi', attn_weights, value)
    wve = jnp.einsum('bhij,dij->bhdi', attn_weights, relative_pos_emb_v)
    # From bhdi to bihd and smash head dim.

    x = (wv + wve).transpose((0, 3, 1, 2))  # From bhdi to bihd.
    # Smash head dim and back to the original inputs dimensions.
    out = x.reshape(bs, input_len, features)
    return nn.GroupNorm(epsilon=1e-4, name='gn_out')(out)


class AxialSelfAttention(nn.Module):
  """Axial Self Attention module.

  Attributes:
    attention_axis: Axis of the attention
    axial_attention_configs: Configurations of the axial attention.
  """

  attention_axis: int
  axial_attention_configs: ml_collections.ConfigDict

  @nn.compact
  def __call__(self, x: jnp.array) -> jnp.ndarray:
    """Applies Axial Self Attention module.

    Args:
      x: Input data.

    Returns:
      Output after axial attention applied on it.
    """
    bs, height, width, channel = x.shape
    if self.attention_axis == 1:  # Row attention.
      x = x.transpose((0, 2, 1, 3)).reshape(bs * width, height, channel)
    elif self.attention_axis == 2:  # Column attention.
      x = x.reshape(bs * height, width, channel)
    else:
      raise ValueError('Only attention over rows or columns is supported.')

    x = SelfAttentionWith1DRelativePos(self.axial_attention_configs.num_heads)(
        x)

    if self.attention_axis == 1:
      return x.reshape((bs, width, height, channel)).transpose((0, 2, 1, 3))
    else:
      return x.reshape((bs, height, width, channel))


class AxialResidualUnit(nn.Module):
  """Bottleneck AxialResNet block.

  Attributes:
    nout: Number of output features.
    axial_attention_configs: Configurations of the axial attention.
    strides: Down-sampling stride.
    bottleneck: If True, the block is a bottleneck block.
  """
  nout: int
  axial_attention_configs: ml_collections.ConfigDict
  strides: Tuple[int, ...] = (1, 1)
  bottleneck: bool = True

  @nn.compact
  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
    features = self.nout
    nout = self.nout * 4 if self.bottleneck else self.nout
    needs_projection = x.shape[-1] != nout or self.strides != (1, 1)
    residual = x
    if needs_projection:
      residual = bit_resnet.StdConv(
          nout, (1, 1), self.strides, use_bias=False, name='conv_proj')(
              residual)
      residual = nn.GroupNorm(epsilon=1e-4, name='gn_proj')(residual)

    if self.bottleneck:
      x = bit_resnet.StdConv(features, (1, 1), use_bias=False, name='conv1')(x)
      x = nn.GroupNorm(epsilon=1e-4, name='gn1')(x)
      x = nn.relu(x)

    # Axial block that is replacing the 3x3 Convs in the ResNet residual unit.
    # Row  attention:
    x = AxialSelfAttention(
        attention_axis=1, axial_attention_configs=self.axial_attention_configs)(
            x)
    # Column attention:
    x = AxialSelfAttention(
        attention_axis=2, axial_attention_configs=self.axial_attention_configs)(
            x)
    if self.strides == (2, 2):
      x = nn.avg_pool(x, (2, 2), strides=(2, 2), padding='SAME')
    x = nn.relu(x)

    last_kernel = (1, 1) if self.bottleneck else (3, 3)
    x = bit_resnet.StdConv(nout, last_kernel, use_bias=False, name='conv3')(x)
    x = nn.GroupNorm(
        epsilon=1e-4, name='gn3', scale_init=nn.initializers.zeros)(
            x)
    x = nn.relu(residual + x)

    return x
